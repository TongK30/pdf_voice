import streamlit as st
import pytesseract
import fitz  # PyMuPDF
from PIL import Image, ImageEnhance, ImageOps
import sys
import shutil
import time
import streamlit.components.v1 as components
import cv2
import numpy as np

# --- C·∫§U H√åNH ---
if sys.platform.startswith('win'):
    PATH_TESSERACT = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
else:
    PATH_TESSERACT = shutil.which("tesseract")

if PATH_TESSERACT:
    pytesseract.pytesseract.tesseract_cmd = PATH_TESSERACT

# --- H√ÄM X·ª¨ L√ù ·∫¢NH (2 CH·∫æ ƒê·ªò) ---
@st.cache_data(show_spinner=False)
def get_page_content_v31(pdf_bytes, page_number, use_opencv=False):
    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        page = doc.load_page(page_number - 1)
        
        # Matrix 1.5: C√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô v√† ƒë·ªô n√©t
        mat = fitz.Matrix(1.5, 1.5) 
        pix = page.get_pixmap(matrix=mat, alpha=False)
        img_pil = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        final_img = img_pil
        
        # --- N·∫æU B·∫¨T OPENCV (CH·∫æ ƒê·ªò L√ÄM N√âT) ---
        if use_opencv:
            # Chuy·ªÉn sang ƒë·ªãnh d·∫°ng OpenCV
            img_np = np.array(img_pil)
            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
            
            # Thu·∫≠t to√°n Adaptive Threshold: T·ª± ƒë·ªông t√°ch ch·ªØ kh·ªèi n·ªÅn
            # Gi√∫p ch·ªØ ti·∫øng Vi·ªát ƒë·∫≠m h∆°n, r√µ d·∫•u h∆°n
            processed = cv2.adaptiveThreshold(
                gray, 255, 
                cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                cv2.THRESH_BINARY, 
                15, 8
            )
            final_img = Image.fromarray(processed)
        else:
            # Ch·∫ø ƒë·ªô th∆∞·ªùng: Ch·ªâ tƒÉng t∆∞∆°ng ph·∫£n nh·∫π
            gray = ImageOps.grayscale(img_pil)
            final_img = ImageEnhance.Contrast(gray).enhance(1.5)

        # OCR
        custom_config = r'--oem 3 --psm 6'
        text = pytesseract.image_to_string(final_img, lang='vie', config=custom_config)
        
        # L√†m s·∫°ch text
        text = text.replace('\n', ' ').replace('|', '').strip()
        
        return img_pil, final_img, text
    except Exception as e:
        return None, None, str(e)

# --- JS ƒê·ªåC T·ª™NG C√ÇU (SMOOTH V30) ---
def mobile_speak_smooth(text):
    safe_text = text.replace('\\', '').replace('"', '').replace("'", "").replace('\n', ' ')
    html = f"""
    <script>
        window.speechSynthesis.cancel();
        
        var fullText = "{safe_text}";
        var sentences = fullText.match(/[^.!?]+[.!?]+|[^.!?]+$/g);
        
        if (!sentences || sentences.length === 0) {{
            sentences = [fullText];
        }}

        var currentIndex = 0;

        function playNextChunk() {{
            if (currentIndex >= sentences.length) {{
                var buttons = window.parent.document.getElementsByTagName('button');
                for (var i = 0; i < buttons.length; i++) {{
                    if (buttons[i].innerText.toUpperCase().includes("TI·∫æP THEO")) {{
                        buttons[i].click(); return;
                    }}
                }}
                return;
            }}

            var chunk = sentences[currentIndex];
            if (!chunk || chunk.trim().length === 0) {{
                currentIndex++; playNextChunk(); return;
            }}

            var msg = new SpeechSynthesisUtterance();
            msg.text = chunk;
            msg.lang = 'vi-VN';
            msg.rate = 1.0; 

            var voices = window.speechSynthesis.getVoices();
            var vn = voices.find(v => v.lang.includes('vi'));
            if (vn) msg.voice = vn;

            msg.onend = function(e) {{
                currentIndex++; playNextChunk(); 
            }};
            
            msg.onerror = function(e) {{
                currentIndex++; playNextChunk();
            }};

            window.speechSynthesis.speak(msg);
        }}
        
        if (window.speechSynthesis.getVoices().length === 0) {{
            window.speechSynthesis.addEventListener('voiceschanged', function() {{ playNextChunk(); }});
        }} else {{
            playNextChunk();
        }}
        
        if (window.speechInterval) clearInterval(window.speechInterval);
        window.speechInterval = setInterval(function() {{
            if (window.speechSynthesis.speaking) {{
                window.speechSynthesis.pause();
                window.speechSynthesis.resume();
            }}
        }}, 8000);
    </script>
    """
    components.html(html, height=0)

# --- GIAO DI·ªÜN CH√çNH ---
st.set_page_config(page_title="PDF OpenCV V31", layout="centered")
st.markdown("<h3 style='text-align: center;'>üì± PDF Pro (OpenCV)</h3>", unsafe_allow_html=True)

uploaded_file = st.file_uploader("Ch·ªçn file PDF:", type="pdf")

if uploaded_file:
    if 'curr_page' not in st.session_state: st.session_state.curr_page = 1
    if 'auto' not in st.session_state: st.session_state.auto = False
    
    # --- C√îNG T·∫ÆC OPENCV ---
    # M·∫∑c ƒë·ªãnh t·∫Øt cho nhanh, c·∫ßn th√¨ b·∫≠t l√™n
    use_opencv = st.checkbox("‚úÖ B·∫≠t ch·∫ø ƒë·ªô l√†m n√©t (OpenCV) - ƒê·ªçc ch·∫≠m nh∆∞ng chu·∫©n h∆°n")

    doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
    total = doc.page_count
    uploaded_file.seek(0)
    bytes_data = uploaded_file.read()

    # Ch·ªçn trang
    st.write("---")
    c_jump, c_label = st.columns([2, 1])
    with c_jump:
        new_page = st.number_input("Trang:", 1, total, st.session_state.curr_page, label_visibility="collapsed")
    with c_label:
        st.write(f"/ {total}")

    if new_page != st.session_state.curr_page:
        st.session_state.curr_page = new_page
        st.rerun()

    # ƒêi·ªÅu h∆∞·ªõng
    c1, c2 = st.columns(2)
    with c1:
        if st.button("‚¨ÖÔ∏è L√ôI L·∫†I", use_container_width=True):
            if st.session_state.curr_page > 1:
                st.session_state.curr_page -= 1
                st.rerun()
    with c2:
        if st.button("TI·∫æP THEO ‚û°Ô∏è", type="primary", use_container_width=True):
            if st.session_state.curr_page < total:
                st.session_state.curr_page += 1
                st.session_state.auto = True
                st.rerun()

    if st.session_state.auto:
        if st.button("üü• D·ª™NG L·∫†I", use_container_width=True):
            components.html("<script>window.speechSynthesis.cancel()</script>", height=0)
            st.session_state.auto = False
            st.rerun()
    else:
        if st.button("‚ñ∂Ô∏è B·∫ÆT ƒê·∫¶U T·ª∞ ƒê·ªòNG", use_container_width=True):
            st.session_state.auto = True
            st.rerun()

    # --- X·ª¨ L√ù ---
    img_org, img_proc, text = get_page_content_v31(bytes_data, st.session_state.curr_page, use_opencv)
    
    # Hi·ªÉn th·ªã ·∫£nh (N·∫øu b·∫≠t OpenCV th√¨ hi·ªán ·∫£nh ƒë√£ x·ª≠ l√Ω ƒë·ªÉ bi·∫øt n√≥ n√©t th·∫ø n√†o)
    if use_opencv and img_proc:
        st.image(img_proc, caption="·∫¢nh ƒë√£ qua OpenCV (Tr·∫Øng ƒëen)", use_container_width=True)
    elif img_org:
        st.image(img_org, caption="·∫¢nh g·ªëc", use_container_width=True)
    
    # Logic ƒê·ªçc
    if st.session_state.auto:
        if text:
            st.toast(f"üîä ƒêang ƒë·ªçc trang {st.session_state.curr_page}...")
            mobile_speak_smooth(text)
            
            with st.expander("Xem ch·ªØ"):
                st.write(text)
        else:
            st.warning("Trang tr·∫Øng. Chuy·ªÉn ti·∫øp...")
            time.sleep(1)
            if st.session_state.curr_page < total:
                st.session_state.curr_page += 1
                st.rerun()
