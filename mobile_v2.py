import streamlit as st
import pytesseract
import fitz  # PyMuPDF
from PIL import Image, ImageEnhance, ImageOps
import sys
import shutil
import time
import streamlit.components.v1 as components
import cv2
import numpy as np

# --- C·∫§U H√åNH ---
if sys.platform.startswith('win'):
    PATH_TESSERACT = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
else:
    PATH_TESSERACT = shutil.which("tesseract")

if PATH_TESSERACT:
    pytesseract.pytesseract.tesseract_cmd = PATH_TESSERACT

# --- H√ÄM X·ª¨ L√ù ·∫¢NH (OPENCV) ---
@st.cache_data(show_spinner=False)
def get_page_v29(pdf_bytes, page_number):
    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        page = doc.load_page(page_number - 1)
        
        mat = fitz.Matrix(1.5, 1.5) 
        pix = page.get_pixmap(matrix=mat, alpha=False)
        img_pil = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        img_np = np.array(img_pil) 
        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
        processed_img = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, 9)
        final_img = Image.fromarray(processed_img)
        
        custom_config = r'--oem 3 --psm 6'
        text = pytesseract.image_to_string(final_img, lang='vie', config=custom_config)
        
        # L√†m s·∫°ch text: X√≥a xu·ªëng d√≤ng th·ª´a, gi·ªØ l·∫°i d·∫•u c√¢u quan tr·ªçng
        clean_text = text.replace('\n', ' ').replace('|', '').strip()
        
        if not clean_text or len(clean_text) < 2:
            return img_pil, "Trang n√†y ch·ªâ c√≥ h√¨nh ·∫£nh."
            
        return img_pil, clean_text
    except Exception as e:
        return None, str(e)

# --- JS CHIA NH·ªé C√ÇU (KH·∫ÆC PH·ª§C L·ªñI D·ª™NG GI·ªÆA CH·ª™NG) ---
def speak_chunks(text):
    # X·ª≠ l√Ω text ƒë·ªÉ JS kh√¥ng l·ªói
    safe_text = text.replace('\\', '').replace('"', '').replace("'", "").replace('\n', ' ')
    
    html = f"""
    <script>
        // 1. H·ªßy l·ªánh c≈©
        window.speechSynthesis.cancel();
        
        // 2. Chia vƒÉn b·∫£n th√†nh c√°c c√¢u nh·ªè (D·ª±a v√†o d·∫•u . ! ? ;)
        // Regex n√†y t√°ch c√¢u nh∆∞ng v·∫´n gi·ªØ l·∫°i d·∫•u c√¢u
        var textContent = "{safe_text}";
        var sentences = textContent.match(/[^.!?]+[.!?]+|[^.!?]+$/g);
        
        if (!sentences || sentences.length === 0) {{
            sentences = [textContent]; // N·∫øu kh√¥ng chia ƒë∆∞·ª£c th√¨ ƒë·ªçc c·∫£ c·ª•c
        }}

        var currentIndex = 0;

        function speakNextSentence() {{
            // N·∫øu ƒë√£ ƒë·ªçc h·∫øt c√°c c√¢u -> B·∫•m Next trang
            if (currentIndex >= sentences.length) {{
                console.log("ƒê√£ ƒë·ªçc h·∫øt trang. Chuy·ªÉn trang...");
                var buttons = window.parent.document.getElementsByTagName('button');
                for (var i = 0; i < buttons.length; i++) {{
                    if (buttons[i].innerText.toUpperCase().includes("TI·∫æP THEO")) {{
                        buttons[i].click();
                        return;
                    }}
                }}
                return;
            }}

            // L·∫•y c√¢u hi·ªán t·∫°i
            var sentence = sentences[currentIndex];
            if (!sentence || sentence.trim().length === 0) {{
                currentIndex++;
                speakNextSentence();
                return;
            }}

            // T·∫°o l·ªánh ƒë·ªçc
            var msg = new SpeechSynthesisUtterance();
            msg.text = sentence;
            msg.lang = 'vi-VN';
            msg.rate = 1.0;

            // T√¨m gi·ªçng
            var voices = window.speechSynthesis.getVoices();
            var vn = voices.find(v => v.lang.includes('vi'));
            if (vn) msg.voice = vn;

            // QUAN TR·ªåNG: Khi ƒë·ªçc xong c√¢u n√†y -> ƒê·ªçc c√¢u ti·∫øp theo
            msg.onend = function(e) {{
                console.log("Xong c√¢u " + currentIndex);
                currentIndex++;
                speakNextSentence(); // ƒê·ªá quy: G·ªçi l·∫°i ch√≠nh n√≥
            }};

            msg.onerror = function(e) {{
                console.log("L·ªói c√¢u " + currentIndex + ", b·ªè qua...");
                currentIndex++;
                speakNextSentence();
            }};

            window.speechSynthesis.speak(msg);
        }}

        // B·∫Øt ƒë·∫ßu quy tr√¨nh
        if (window.speechSynthesis.getVoices().length === 0) {{
            window.speechSynthesis.addEventListener('voiceschanged', function() {{
                speakNextSentence();
            }});
        }} else {{
            speakNextSentence();
        }}
        
        // Anti-Sleep (Gi·ªØ cho tr√¨nh duy·ªát kh√¥ng ng·ªß g·∫≠t)
        if (window.speechInterval) clearInterval(window.speechInterval);
        window.speechInterval = setInterval(function() {{
            if (window.speechSynthesis.speaking) {{
                window.speechSynthesis.pause();
                window.speechSynthesis.resume();
            }}
        }}, 8000);

    </script>
    """
    components.html(html, height=0)

# --- GIAO DI·ªÜN ---
st.set_page_config(page_title="PDF Chunk Reader", layout="centered")
st.markdown("<h3 style='text-align: center;'>üìñ ƒê·ªçc PDF (Kh√¥ng bao gi·ªù ng·∫Øt)</h3>", unsafe_allow_html=True)

uploaded_file = st.file_uploader("Upload PDF:", type="pdf")

if uploaded_file:
    if 'curr_page' not in st.session_state: st.session_state.curr_page = 1
    if 'auto' not in st.session_state: st.session_state.auto = False

    doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
    total = doc.page_count
    uploaded_file.seek(0)
    bytes_data = uploaded_file.read()

    # Ch·ªçn trang
    st.write("---")
    c_jump, c_label = st.columns([2, 1])
    with c_jump:
        new_page = st.number_input("Trang:", 1, total, st.session_state.curr_page, label_visibility="collapsed")
    with c_label:
        st.write(f"/ {total}")

    if new_page != st.session_state.curr_page:
        st.session_state.curr_page = new_page
        st.rerun()

    # N√∫t b·∫•m
    c1, c2 = st.columns(2)
    with c1:
        if st.button("‚¨ÖÔ∏è BACK", use_container_width=True):
            if st.session_state.curr_page > 1:
                st.session_state.curr_page -= 1
                st.rerun()
    with c2:
        if st.button("TI·∫æP THEO ‚û°Ô∏è", type="primary", use_container_width=True):
            if st.session_state.curr_page < total:
                st.session_state.curr_page += 1
                st.session_state.auto = True
                st.rerun()

    if st.session_state.auto:
        if st.button("üü• D·ª™NG L·∫†I", use_container_width=True):
            components.html("<script>window.speechSynthesis.cancel()</script>", height=0)
            st.session_state.auto = False
            st.rerun()
    else:
        if st.button("‚ñ∂Ô∏è B·∫ÆT ƒê·∫¶U ƒê·ªåC", use_container_width=True):
            st.session_state.auto = True
            st.rerun()

    # X·ª≠ l√Ω & Hi·ªÉn th·ªã
    img, text = get_page_v29(bytes_data, st.session_state.curr_page)
    
    if img: st.image(img, use_container_width=True)
    
    st.info("üìù VƒÉn b·∫£n ƒëang x·ª≠ l√Ω:")
    st.text_area("", text, height=100, label_visibility="collapsed")

    if st.session_state.auto:
        st.toast(f"üîä ƒêang ƒë·ªçc t·ª´ng c√¢u trang {st.session_state.curr_page}...")
        speak_chunks(text)
